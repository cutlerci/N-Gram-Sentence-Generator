![VCU Logo](https://ocpe.vcu.edu/media/ocpe/images/logos/bm_CollEng_CompSci_RF2_hz_4c.png)

# Cutlerci-CMSC416-Project 2
My name is Charles Ian Cutler, CIC, currently enrolled in the College of Engineering at Virginia Commonwealth Univeristy. 
This repository is for a project in Virginia Commonwealth University course CMSC 416, Introduction Natural Language Processing, Spring 2022.
## Repository Files
1) ngram.py -- The program itself.
## N-GRAM Description
 The following program is for the second programming assignment in the course CMSC 416, Intro to Natural Language Processing, at Virginia Commonwealth University

This program was created with the intention of learning how an N-Gram language model can be used to generate sentences. The idea was that for this assignment I would read in a number of plain text files and use these files to "train" a model. This model would be used to probabilistically generate sentences. Specifically, the sentences would be created from a start tag as the "seed" and from this seed the sentence would "grow" based on the probability that a given word might follow the previous word. In fact the previous word need not be isolated to a singular word, but can be a previous set of words.

The program is ngram.py, and it should be run from the command line with a minimum of 3 arguments. The first argument is denoted "n" in the below command prompt sample. This "n" represents the number for the N-Gram model, where a 1 would be an Unigram, 2 would be a Bigram, and so on. Essentially, "n" represents the size of the previous set of words to be considered when determine the next word that is likely to be in the sentence being generated. The second argument is denoted "m" in the below command prompt sample. This "m" represents the number of sentences the user would like to have generated by the program after the model has been trained. The final argument or arguments are the plain text files for the program to "read". These files should be a list of one or more file names from which the model will be built from.

COMMAND PROMPT SAMPLE: >>> ngram.py n m input1.txt input2.txt input3.txt

After the program has been run by a command call as described above, the program will read the text files, and then output the requested number of sentences.

In the following sample run I used the following novels to train the model with ~ 7.2 million words.
   - The Adventures of Sherlock Holmes, Sir Arthur Conan Doyle - 105,071 words
   - Ulysses, James Joyce - 265,222 words
   - Bleak House, Charles Dickens - 360,947 words
   - Great Expectations, Charles Dickens - 183,349 words
   - Les Miserables, Victor Hugo - 530,982 words
   - War and Peace, Leo Tolstoy - 561,304 words
   - Anna Karenina, Leo Tolstoy - 349,736 words
   - Moby Dick, Herman Melville - 206,052 words
   - Jane Eyre, Charlotte Brontë - 183,858 words
   - Sense and Sensibility, Jane Austen - 126,194 words
   - Crime and Punishment, Fyodor Dostoyevsky - 211,591 words
   - The Grapes of Wrath, John Steinbeck - 169,481 words
   - The tale of Two Cities, Charles Dickens - 135,420 words
   - Frankenstein, Mary Shelley - 75,380 words
   - Alice’s Adventures In Wonderland, Lewis Carroll - 29,610 words
   - Wizard of Oz, Frank L. Baum  - 40,508 words
   - Dorothy and the Wizard in Oz, Frank L. Baum - 46,000 words
   - Ozma of Oz, Frank L. Baum - 68,000 words
   - Glinda of Oz, Frank L. Baum - 76,000 words
   - The Magic of Oz, Frank L. Baum - 40,508 words
   - The Emerald City of Oz, Frank L. Baum - 42,006 words
   - Tik-Tok of Oz, Frank L. Baum - 52,828 words
   - The Lost Princess of Oz, Frank L. Baum - 88,000 words
   - The Patchwork Girl of Oz, Frank L. Baum - 88,000 words
   - The Road to Oz, Frank L. Baum - 68,000 words
   - The Tin Woodman of Oz, Frank L. Baum - 47,915 words
   - Rinkitink in Oz, Frank L. Baum - 30,000 words
   - Great Gatsby, F. Scott Fitzgerald - 47,094 words
   - The Scarlet Letter, Nathaniel Hawthorne - 87,000 words
   - A Modest Proposal, Jonathan Swift - 15,000 words
   - Dracula, Bram Stoker - 96,000 words
   - The Prince, Niccolò Machiavelli - 47,500 words
   - The Illiad, Homer - 193,536 words
   - Don Quixote, Miguel de Cervantes - 430,269 words
   - Monte Cristo, Alexandre Dumas - 464,162 words
   - David Copperfield, Charles Dickens - 360231 words
   - Republic, Plato - 220117 words
   - Leviathan, Thomas Hobbes - 216349 words
   - Collection Of Dr. Seuss, Theodore Geisal, 10,000 words

<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>

\>\>\> ngram.py 3 20 (For clarity, I will refrain from listing the above texts a second time.)
 Holmes shook his head because he couldn't count the pesetas and the choice, if it must be the remains of the union jack, his eyelashes are moistened with emotion.

 For my youth, but had so wished to do in moscow, concealing his name as the sudden change, but it’s only to give my hand and went for myself I know that, to express them— be not true, because up to the grandeur and gives me life.

 Thus while he was unhappy because he is down against the dull cracked windows in it, so I suppose your brother and his family as well as of the pillory, a wise old dogs lay down at once, gavroche, pushing his way up as I am thankful I escaped from the outside.

 The man who has broken him down.

 He replied, and I stole her kiss.

 Far be the foregone evil what it all is over.

 The man was a sight of the field was deserted.

 Some fifteen men with their glittering swords.

 Anna and I could not fancy ourselves so vastly superior to the brain which is not well, mr morris on the panelled wall and mounted the stairs.

 I have the most natural thing would fret her, and at once she began to turn itself round and round with a funny ornament that might denote suspicion or not.

 Corny kelleher said.

 The child in contact with them.
 In my chest and lifted his head you say, and he will make the most light-hearted mood.

 He, however, muttered general clock, blusteringly, for he had replaced them with horror and loathing.

 Murmurs of the sounds that came into the village had taken to the experienced prince atrides cried he why, m.

 His teachers.

 To devote my life to you, you cannot be ashamed of yourself.

 Anyone would know that I could hear him talk of that country.

 This said, pisander, from the firm athenian phalanx stands, and gasps at him.

 Pierre said was true, he shouted again, receiving no orders, but only spoke but thought, and as she walked up to your commands.

<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>

The program works by first parsing in the command line arguments. After which the text file is read in and stored as a single string variable. This string variable, containing the entire file is preprocessed resulting in the following modifications:
 - The entire string is made entirely lower case.
 - The words " Mr. Mrs. Dr. have their periods stripped as to not be mistaken as the end of a sentence.
 - The punctuation denoting the end of sentence, (. ! ?) are replaced with an "End Tag" followed by a number of start tags to denote the start of the next sentence.
 - Then a number of characters I decided to remove are replaced stripped from the string.
Finally, the string is split at the white space and stored in a list with each word in its own index in the list. The next step is to generate the N-Gram model data. Two dictionaries are filled with the appropriate frequency counts of 1) How many times we have seen a previous phrase and 2) how many times we have seen a specific word preceded by that previous phrase (referred to in the code as the words history).

Lastly, the sentences are generated using a random number generated and probabilities calculated from the N-Gram frequency counts. The "n-1" previous words in the generated sentence are the history, and the chance that a word is picked depends on the probability value. To pick a word, the probability total is compared against the random number. As the program iterates through possible words, their probability value is added to the total. When the total value is greater than the random number the program picks that word. This concept can be visualized as traversing down a number line where the size of each segment of the number line is the probability of that word given the "history". As we traverse toward the line and sum the numbers we approach the number that was randomly generate. When the random number is surpassed then we know we have entered the segment of the number line denoting the word to be picked.

I used resources provided by Dr. Bridget McInnes of Virgina Commonwealth University to understand how to use regular expressions in python 3. Additionally I used a guide provided by her to learn how to take inputs from the command line and to iterate over dictionaries in python 3.

Additionally, I used the website "GEEKS FOR GEEKS", https://www.geeksforgeeks.org/ to further my understanding of dictionaries and the collections class in python.

No code was copied from either source.

This code is property of CHARLES I CUTLER, student at Virginia Commonwealth University.
