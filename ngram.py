# Name: Charles Ian Cutler
# Date: 02/20/2022
# Class: CMSC 416 Introduction to Natural Language Processing
# The following program is for the second programming assignment in the course
# CMSC 416, Intro to Natural Language Processing, at Virginia Commonwealth University
#
# This program was created with the intention of learning how an N-Gram language model
# can be used to generate sentences. The idea was that for this assignment I would read in a number of
# plain text files and use these files to "train" a model. This model would be used to probabilistically
# generate sentences. Specifically, the sentences would be created from a start tag as the "seed" and from
# this seed the sentence would "grow" based on the probability that a given word might follow the previous word.
# In fact the previous word need not be isolated to a singular word, but can be a previous set of words.
#
# The program is ngram.py, and it should be run from the command line with a minimum of 3 arguments.
# The first argument is denoted "n" in the below command prompt sample. This "n" represents the number for
# the N-Gram model, where a 1 would be an Unigram, 2 would be a Bigram, and so on. Essentially, "n" represents
# the size of the previous set of words to be considered when determine the next word that is likely
# to be in the sentence being generated.
# The second argument is denoted "m" in the below command prompt sample. This "m" represents the number of
# sentences the user would like to have generated by the program after the model has been trained.
# The final argument or arguments are the plain text files for the program to "read".
# These files should be a list of one or more file names from which the model will be built from.
#
# COMMAND PROMPT SAMPLE: >>> ngram.py n m input1.txt input2.txt input3.txt
#
# After the program has been run by a command call as described above,
# the program will read the text files, and then output the requested number of sentences.
#
# In the following sample run I used the following novels to train the model with ~ 7.2 million words.
#   - The Adventures of Sherlock Holmes, Sir Arthur Conan Doyle - 105,071 words
#   - Ulysses, James Joyce - 265,222 words
#   - Bleak House, Charles Dickens - 360,947 words
#   - Great Expectations, Charles Dickens - 183,349 words
#   - Les Miserables, Victor Hugo - 530,982 words
#   - War and Peace, Leo Tolstoy - 561,304 words
#   - Anna Karenina, Leo Tolstoy - 349,736 words
#   - Moby Dick, Herman Melville - 206,052 words
#   - Jane Eyre, Charlotte Brontë - 183,858 words
#   - Sense and Sensibility, Jane Austen - 126,194 words
#   - Crime and Punishment, Fyodor Dostoyevsky - 211,591 words
#   - The Grapes of Wrath, John Steinbeck - 169,481 words
#   - The tale of Two Cities, Charles Dickens - 135,420 words
#   - Frankenstein, Mary Shelley - 75,380 words
#   - Alice’s Adventures In Wonderland, Lewis Carroll - 29,610 words
#   - Wizard of Oz, Frank L. Baum  - 40,508 words
#   - Dorothy and the Wizard in Oz, Frank L. Baum - 46,000 words
#   - Ozma of Oz, Frank L. Baum - 68,000 words
#   - Glinda of Oz, Frank L. Baum - 76,000 words
#   - The Magic of Oz, Frank L. Baum - 40,508 words
#   - The Emerald City of Oz, Frank L. Baum - 42,006 words
#   - Tik-Tok of Oz, Frank L. Baum - 52,828 words
#   - The Lost Princess of Oz, Frank L. Baum - 88,000 words
#   - The Patchwork Girl of Oz, Frank L. Baum - 88,000 words
#   - The Road to Oz, Frank L. Baum - 68,000 words
#   - The Tin Woodman of Oz, Frank L. Baum - 47,915 words
#   - Rinkitink in Oz, Frank L. Baum - 30,000 words
#   - Great Gatsby, F. Scott Fitzgerald - 47,094 words
#   - The Scarlet Letter, Nathaniel Hawthorne - 87,000 words
#   - A Modest Proposal, Jonathan Swift - 15,000 words
#   - Dracula, Bram Stoker - 96,000 words
#   - The Prince, Niccolò Machiavelli - 47,500 words
#   - The Illiad, Homer - 193,536 words
#   - Don Quixote, Miguel de Cervantes - 430,269 words
#   - Monte Cristo, Alexandre Dumas - 464,162 words
#   - David Copperfield, Charles Dickens - 360231 words
#   - Republic, Plato - 220117 words
#   - Leviathan, Thomas Hobbes - 216349 words
#   - Collection Of Dr. Seuss, Theodore Geisal, 10,000 words
#
# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
# >>> ngram.py 3 20 [For clarity, I will refrain from listing the above texts a second time.]
# Holmes shook his head because he couldn't count the pesetas and the choice, if it must be the remains of the union
# jack, his eyelashes are moistened with emotion.
#
# For my youth, but had so wished to do in moscow, concealing his name as the sudden change, but it’s only to give my
# hand and went for myself I know that, to express them— be not true, because up to the grandeur and gives me life.
#
# Thus while he was unhappy because he is down against the dull cracked windows in it, so I suppose your brother and
# his family as well as of the pillory, a wise old dogs lay down at once, gavroche, pushing his way up as I am
# thankful I escaped from the outside.
#
# The man who has broken him down.
#
# He replied, and I stole her kiss.
#
# Far be the foregone evil what it all is over.
#
# The man was a sight of the field was deserted.
#
# Some fifteen men with their glittering swords.
#
# Anna and I could not fancy ourselves so vastly superior to the brain which is not well, mr morris on the panelled
# wall and mounted the stairs.
#
# I have the most natural thing would fret her, and at once she began to turn itself round and round with a funny
# ornament that might denote suspicion or not.
#
# Corny kelleher said.
#
# The child in contact with them.
# In my chest and lifted his head you say, and he will make the most light-hearted mood.
#
# He, however, muttered general clock, blusteringly, for he had replaced them with horror and loathing.
#
# Murmurs of the sounds that came into the village had taken to the experienced prince atrides cried he why, m.
#
# His teachers.
#
# To devote my life to you, you cannot be ashamed of yourself.
#
# Anyone would know that I could hear him talk of that country.
#
# This said, pisander, from the firm athenian phalanx stands, and gasps at him.
#
# Pierre said was true, he shouted again, receiving no orders, but only spoke but thought, and as she walked up to
# your commands.
# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
#
# The program works by first parsing in the command line arguments.
# After which the text file is read in and stored as a single string variable. This string variable,
# containing the entire file is preprocessed resulting in the following modifications:
# - The entire string is made entirely lower case.
# - The words " Mr. Mrs. Dr. have their periods stripped as to not be mistaken as the end of a sentence.
# - The punctuation denoting the end of sentence, (. ! ?) are replaced with an "End Tag" followed by a
#   a number of start tags to denote the start of the next sentence.
# - Then a number of characters I decided to remove are replaced stripped from the string.
# Finally, the string is split at the white space and stored in a list with each word in its own index in the list.
# The next step is to generate the N-Gram model data. Two dictionaries are filled with the appropriate frequency
# counts of 1) How many times we have seen a previous phrase and 2) how many times we have seen a specific
# word preceded by that previous phrase (referred to in the code as the words history).
#
# Lastly, the sentences are generated using a random number generated and probabilities
# calculated from the N-Gram frequency counts. The "n-1" previous words in the generated sentence
# are the history, and the chance that a word is picked depends on the probability value. To pick a word,
# the probability total is compared against the random number. As the program iterates through possible
# words, their probability value is added to the total. When the total value is greater than the random number
# the program picks that word. This concept can be visualized as traversing down a number line where the size of each
# segment of the number line is the probability of that word given the "history".
# As we traverse toward the line and sum the numbers we approach the number that was randomly generate.
# When the random number is surpassed then we know we have entered the segment of the number line
# denoting the word to be picked.
#
# I used resources provided by Dr. Bridget McInnes of Virgina Commonwealth University to understand how to use
# regular expressions in python 3. Additionally I used a guide provided by her to learn how to take inputs from
# the command line and to iterate over dictionaries in python 3.
#
# Additionally, I used the website "GEEKS FOR GEEKS", https://www.geeksforgeeks.org/ to further my understanding of
# dictionaries and the collections class in python.
#
# No code was copied from either source.
#
# This code is property of CHARLES I CUTLER,
# student at Virginia Commonwealth University.

from sys import argv
import re
import random
from collections import defaultdict


# The following function is used to print the output sentences in a more human-readable format.
# The sentences are stripped of the start and end tags as well as the trailing extra space.
# Then the sentence is capitalized and any "i" characters in the sentence are made uppercase.
# Then the sentence is printed to the screen.


def print_sentence(param):
    param = re.sub(r"<start>", "", param)
    param = re.sub(r"\s<end>", ".", param)
    param = param.lstrip()
    param = param.capitalize()
    param = re.sub(r"\si\s", " I ", param)
    print(param)
    print()


ngramNumber = int(argv[1])
sentencesNeededNumber = int(argv[2])
fileNames = argv[3:]
historyDict = defaultdict(int)
nGramDict = defaultdict(dict)
history = ""
totalWordsInAllBooks = 0

print("This program generates random sentences based on an N-Gram model.")
print("Command line settings : ngram.py " + str(ngramNumber) + " " + str(sentencesNeededNumber))
print("Author: Charles Cutler")

# Make the start variable
startVariable = ""
if ngramNumber == 1:
    startVariable = "<start> "
else:
    for _ in range(ngramNumber-1):
        startVariable += "<start> "


# Get an entire book, or corpus, as a string
for textFile in fileNames:
    nGramList = []
    with open(textFile, 'r') as file:
        corpusString = file.read().replace('\n', " ")  # Whole thing is a string
        file.close()

    # Mutate the book to have the tags, preprocess certain characters, and then make it an array of individual words
    corpusString = corpusString.lower()
    corpusString = re.sub(r"mr\.", "mr", corpusString)
    corpusString = re.sub(r"mrs\.", "mrs", corpusString)
    corpusString = re.sub(r"dr\.", "dr", corpusString)
    corpusString = re.sub(r"[.?!]", " <end> " + startVariable, corpusString)
    corpusString = re.sub(r"[“”\":;_)(\[\]*]", "", corpusString)
    corpusString = re.sub(r"\[Illustration]", "", corpusString)
    corpusString = re.sub(r"\.\.\.", ".", corpusString)
    corpusString = re.sub(r",", " , ", corpusString)
    corpusString = startVariable + corpusString.rstrip(startVariable) + ">"
    wordArray = corpusString.split()
    totalWordsInAllBooks += len(wordArray)

    # If we want to use Uni-grams, just count the frequency of each word individually.
    if ngramNumber == 1:
        for word in wordArray:
            historyDict[word] += 1
    else:
        for word in wordArray:
            nGramList.append(word)  # Add word to sliding window of length n (the N-Gram)
            if len(nGramList) == ngramNumber:  # If the sliding window is full, that is we have the full N-Gram
                keyWord = nGramList.pop(-1)  # Pop off the last word in the window
                for x in nGramList:  # concatenate all the remaining (n-1) words in the window as the "history"
                    history += (x + " ")
                history = history.rstrip(" ")  # remove the trailing space on the history
                historyDict[history] += 1  # This updates the Frequency we have seen the history phrase

                # This updates the frequency we have seen the history phrase followed by the specific keyWord
                nGramDict[history][keyWord] = (nGramDict.setdefault(history, {})).setdefault(keyWord, 0) + 1

                nGramList.append(keyWord)  # This reinstates the N-Gram window.
                nGramList.pop(0)  # This "slides" the window one to the right.
                history = ""  # This resets the history phrase for the next iteration.

print("Total words read: " + str(totalWordsInAllBooks) + "\n")


randomNum = random.random()
probSum = 0
update = []
currentHistoryDict = {}

# Build the N-Gram tables (uni gram, and history dict)
for x in range(sentencesNeededNumber, 0, -1):
    sentence = ""
    currentWord = ""
    history = startVariable.rstrip()  # Starts the first history as the start tag minus the trailing space.

    while currentWord != "<end>":  # Make a sentence while the program has not selected
        # the end tag which denotes that the sentence is complete

        # if wwe want to generate sentences from a unigram the program enters the following if
        if ngramNumber == 1:
            currentHistoryDict = historyDict  # in the case this line is reached, the history dict is an unigram table
            # For all the word in the corpus, consider one at a time if a given word will be the randomly selected word
            # to be put in the sentence.
            # Then this process is repeated until the end tag is picked
            for key, values in currentHistoryDict.items():
                probSum += (historyDict[history])/totalWordsInAllBooks
                if randomNum < probSum:
                    currentWord = key
                    sentence += (" " + currentWord)
                    break
            # This line here "slides" the history one word further along the sentence.
            history = currentWord
            # This resets the numerical values for the next word.
            probSum = 0
            randomNum = random.random()
        else:
            # The below line retrieves the nested dictionary for that specific history value.
            # This is used to travers all the possible options for the next word in the sentence.
            currentHistoryDict = nGramDict[history]

            # The following for loop checks to see if the random number value has been reaching
            # while "traversing the number line" as described in the opening comments.
            # When the sum of the probabilities so far has been reached then a word has been
            # selected and added to the sentence.
            for key, values in currentHistoryDict.items():
                probSum += (nGramDict[history][key])/(historyDict[history])
                if randomNum < probSum:
                    currentWord = key
                    sentence += (" " + currentWord)
                    break
            # This Section here "slides" the history one word further along the sentence
            # after the next word has been selected.
            update = history.split()
            history = ""
            update.pop(0)
            update.append(currentWord)
            for word in update:
                history += word + " "
            history = history.rstrip()

            # This resets the numerical values for the next word.
            probSum = 0
            randomNum = random.random()
    # This is a special function I built to print the sentences in a cleaner, more "correct" format for reading.
    print_sentence(sentence)
